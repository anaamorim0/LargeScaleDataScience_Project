{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXwanky781fI"
   },
   "source": [
    "# Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOjwWsiv81fJ"
   },
   "source": [
    "### Conducted by: </br>\n",
    "        Ana Francisca Rocha (202208946)\n",
    "        Ana Amorim (2022)\n",
    "        Pedro Rufino (202208600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHtxoXv81fJ"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aty7P_1N81fJ"
   },
   "source": [
    "This notebook aims to evaluate the performance of several Python libraries for processing large-scale datasets, as part of the Large-Scale Data Science course.\n",
    "\n",
    "The work is divided into two main phases:\n",
    "\n",
    "- In the first phase, we reproduce an experiment comparing basic operations between Koalas (PySpark) and Dask.\n",
    "\n",
    "- In the second phase, we extend the analysis by including additional libraries (Modin, JobLib, and RAPIDS) and testing different combinations on the large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcGZUio881fK"
   },
   "source": [
    "## Experiment of \"NYC taxi driver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL7OBbXc81fK"
   },
   "source": [
    "Source: https://www.databricks.com/blog/2021/04/07/benchmark-koalas-pyspark-and-dask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:52:17.981408Z",
     "start_time": "2024-06-03T09:52:17.975900Z"
    },
    "id": "eayv1Lv681fK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:52:18.463310Z",
     "start_time": "2024-06-03T09:52:18.123271Z"
    },
    "id": "alSEwal381fL",
    "outputId": "5162c089-dcff-424c-c940-d52bf485ec9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 2.2.3\n",
      "numpy version: 2.2.6\n",
      "pyspark version: 4.0.0\n",
      "dask version: 2025.5.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "print('pandas version: %s' % pd.__version__)\n",
    "print('numpy version: %s' % np.__version__)\n",
    "print('pyspark version: %s' % pyspark.__version__)\n",
    "print('dask version: %s' % dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvPmoabA81fL"
   },
   "source": [
    "#### Regular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4oZKImIz81fM"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Cannot assign requested address: bind: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\r\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\r\n\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\r\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\r\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\r\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:561)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1281)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\r\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:922)\r\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:259)\r\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRead Parquet Example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen_tripdata_2015-01.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(parquet_file)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:207\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    205\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:300\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:429\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1627\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1621\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1623\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1624\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1626\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1627\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Cannot assign requested address: bind: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\r\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\r\n\tat java.base/sun.nio.ch.Net.bind(Net.java:565)\r\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)\r\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)\r\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:561)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1281)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\r\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:922)\r\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:259)\r\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:380)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:173)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:166)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "parquet_file = \"green_tripdata_2015-01.parquet\"\n",
    "data = spark.read.parquet(parquet_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yljdTMTW81fM",
    "outputId": "34743366-3999-4590-c502-d63ba5d349af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VendorID', 'bigint'),\n",
       " ('lpep_pickup_datetime', 'timestamp_ntz'),\n",
       " ('lpep_dropoff_datetime', 'timestamp_ntz'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('RatecodeID', 'bigint'),\n",
       " ('PULocationID', 'bigint'),\n",
       " ('DOLocationID', 'bigint'),\n",
       " ('passenger_count', 'bigint'),\n",
       " ('trip_distance', 'double'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('ehail_fee', 'int'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('payment_type', 'bigint'),\n",
       " ('trip_type', 'double'),\n",
       " ('congestion_surcharge', 'int')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wgKC3E781fM",
    "outputId": "da0b25db-6a4f-4867-9817-df5979bb391c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1508493\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v47W_ZCY81fM",
    "outputId": "8b8a6a32-3c9e-4318-a185-46554f5a3441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data is 33.7250% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (data['tip_amount'] >= 1) & (data['tip_amount'] <= 5)\n",
    "\n",
    "filtered_data = data.filter(expr_filter)\n",
    "percentage = (filtered_data.count() / data.count()) * 100\n",
    "\n",
    "print(f'Filtered data is {percentage:.4f}% of total data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHokZLXp81fM"
   },
   "source": [
    "#### Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j42P7PKY81fN"
   },
   "outputs": [],
   "source": [
    "large_df_url = \"yellow_tripdata_2011-*.parquet\"\n",
    "large_data = spark.read.parquet(large_df_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ8iY4yh81fN",
    "outputId": "685a0039-39f1-4272-ead8-08899a3bad89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VendorID', 'bigint'),\n",
       " ('tpep_pickup_datetime', 'timestamp_ntz'),\n",
       " ('tpep_dropoff_datetime', 'timestamp_ntz'),\n",
       " ('passenger_count', 'bigint'),\n",
       " ('trip_distance', 'double'),\n",
       " ('RatecodeID', 'bigint'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('PULocationID', 'bigint'),\n",
       " ('DOLocationID', 'bigint'),\n",
       " ('payment_type', 'bigint'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('congestion_surcharge', 'double'),\n",
       " ('airport_fee', 'double')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY_RKrCb81fN",
    "outputId": "958bd144-413e-447c-c0df-ae69f5e1ca7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 13464997\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", large_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A9n-qpO81fN",
    "outputId": "cf71d34a-d8cd-4704-cf1e-3769a7426e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data is 35.8422% of total data\n"
     ]
    }
   ],
   "source": [
    "filtered_data = large_data.filter((col(\"tip_amount\") >= 1) & (col(\"tip_amount\") <= 5))\n",
    "\n",
    "total_count = large_data.count()\n",
    "filtered_count = filtered_data.count()\n",
    "percentage = (filtered_count / total_count) * 100\n",
    "\n",
    "print(f\"Filtered data is {percentage:.4f}% of total data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vr5SwgUg81fN"
   },
   "source": [
    "#### Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnbtxgtT81fN"
   },
   "outputs": [],
   "source": [
    "url = \"yellow_tripdata_2012-01.parquet\"\n",
    "data = pd.read_parquet(url)\n",
    "small_data = data.head(50000)\n",
    "small_data.to_parquet('yellow_tripdata_2012-01_subset-50000.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsXLPYs981fN"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Parquet Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "small_df_url = 'yellow_tripdata_2012-01_subset-50000.parquet'\n",
    "small_data = spark.read.parquet(small_df_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As48ZK4y81fO",
    "outputId": "1087bffe-4e31-42cc-ab31-6971c8cea7ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VendorID', 'bigint'),\n",
       " ('tpep_pickup_datetime', 'timestamp_ntz'),\n",
       " ('tpep_dropoff_datetime', 'timestamp_ntz'),\n",
       " ('passenger_count', 'bigint'),\n",
       " ('trip_distance', 'double'),\n",
       " ('RatecodeID', 'bigint'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('PULocationID', 'bigint'),\n",
       " ('DOLocationID', 'bigint'),\n",
       " ('payment_type', 'bigint'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('improvement_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('congestion_surcharge', 'int'),\n",
       " ('airport_fee', 'int')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR4ekjdC81fO",
    "outputId": "8b9b15e5-af24-4ab6-8b80-607e801b15f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", small_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSjCKjR481fO",
    "outputId": "6fe32f98-0739-4e52-b298-33d01017d2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data is 30.2000% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (small_data['tip_amount'] >= 1) & (small_data['tip_amount'] <= 5)\n",
    "\n",
    "filtered_data = small_data.filter(expr_filter)\n",
    "percentage = (filtered_data.count() / small_data.count()) * 100\n",
    "\n",
    "print(f'Filtered data is {percentage:.4f}% of total data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4Ri3eNv81fO"
   },
   "outputs": [],
   "source": [
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.monotonic() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "\n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QLbSoEX81fO"
   },
   "source": [
    "### Koalas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqJt2pOx81fO"
   },
   "source": [
    "#### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UEyXx-281fO"
   },
   "outputs": [],
   "source": [
    "def count(df=None):\n",
    "    return df.count()\n",
    "\n",
    "def mean(df):\n",
    "    return df.select(F.mean(\"fare_amount\")).collect()[0][0]\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return float(df.select(F.stddev(\"fare_amount\")).collect()[0][0])\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return float(df.select((F.col(\"fare_amount\") + F.col(\"tip_amount\")).alias(\"total\")).agg(F.mean(\"total\")).collect()[0][0])\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return float(df.select((F.col(\"fare_amount\") * F.col(\"tip_amount\")).alias(\"product\")).agg(F.mean(\"product\")).collect()[0][0])\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.groupBy(\"fare_amount\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret.mean()\n",
    "\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupBy(\"passenger_count\").agg(\n",
    "        F.mean(\"fare_amount\").alias(\"fare_amount_mean\"),\n",
    "        F.stddev(\"fare_amount\").alias(\"fare_amount_std\"),\n",
    "        F.mean(\"tip_amount\").alias(\"tip_amount_mean\"),\n",
    "        F.stddev(\"tip_amount\").alias(\"tip_amount_std\")\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "def join_count(df, other):\n",
    "    return df.join(other.hint(\"broadcast\"), on=\"passenger_count\").count()\n",
    "\n",
    "def join_data(df, other):\n",
    "    return df.join(other.hint(\"broadcast\"), on=\"passenger_count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0cZK9uS81fP"
   },
   "source": [
    "##### Regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrQUjO_O81fP"
   },
   "outputs": [],
   "source": [
    "koalas_regular_df_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}\n",
    "\n",
    "regular_df_url = \"green_tripdata_2015-01.parquet\"\n",
    "koalas_data = spark.read.parquet(regular_df_url)\n",
    "\n",
    "grouped_stats = groupby_statistics(koalas_data)\n",
    "other_pd = grouped_stats.toPandas()\n",
    "other = spark.createDataFrame(other_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws598s0w81fP"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(regular_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sgtIg6h81fP",
    "outputId": "862ad6a3-2828-4af8-b687-09fb0aeefe45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.06300000000010186 seconds\n",
      "count took: 0.125 seconds\n",
      "mean took: 0.23399999999946886 seconds\n",
      "standard deviation took: 0.25 seconds\n",
      "mean of columns addition took: 0.2820000000001528 seconds\n",
      "addition of columns took: 0.0 seconds\n",
      "mean of columns multiplication took: 0.31199999999989814 seconds\n",
      "multiplication of columns took: 0.0 seconds\n",
      "value counts took: 0.03099999999994907 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.04700000000048021 seconds\n",
      "join count took: 13.234999999999673 seconds\n",
      "join took: 0.015000000000327418 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=koalas_regular_df_benchmarks, name='read file')\n",
    "benchmark(count, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='count')\n",
    "benchmark(mean, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=koalas_data, benchmarks=koalas_regular_df_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, koalas_data, benchmarks=koalas_regular_df_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, koalas_data, benchmarks=koalas_regular_df_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaGi1pzt81fP"
   },
   "source": [
    "##### Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr8EJ1lO81fP"
   },
   "outputs": [],
   "source": [
    "koalas_small_df_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}\n",
    "\n",
    "small_df_url = \"yellow_tripdata_2012-01_subset-50000.parquet\"\n",
    "koalas_data = spark.read.parquet(small_df_url)\n",
    "\n",
    "grouped_stats = groupby_statistics(koalas_data)\n",
    "other_small_pd = grouped_stats.toPandas()\n",
    "other_small = spark.createDataFrame(other_small_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz-o75EJ81fP"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(small_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouoMnBxk81fP",
    "outputId": "9b1d0703-be30-4bb0-da1f-d66d1e14110b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.10900000000037835 seconds\n",
      "count took: 0.09400000000005093 seconds\n",
      "mean took: 0.125 seconds\n",
      "standard deviation took: 0.17199999999957072 seconds\n",
      "mean of columns addition took: 0.14000000000032742 seconds\n",
      "addition of columns took: 0.0 seconds\n",
      "mean of columns multiplication took: 0.14099999999962165 seconds\n",
      "multiplication of columns took: 0.0 seconds\n",
      "value counts took: 0.016000000000531145 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.03099999999994907 seconds\n",
      "join count took: 11.140999999999622 seconds\n",
      "join took: 0.015000000000327418 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=koalas_small_df_benchmarks, name='read file')\n",
    "benchmark(count, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='count')\n",
    "benchmark(mean, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=koalas_data, benchmarks=koalas_small_df_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, koalas_data, benchmarks=koalas_small_df_benchmarks, name='join count', other=other_small)\n",
    "benchmark(join_data, koalas_data, benchmarks=koalas_small_df_benchmarks, name='join', other=other_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIrgEkQ981fQ"
   },
   "source": [
    "##### Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNyAMtBI81fQ"
   },
   "outputs": [],
   "source": [
    "koalas_large_df_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}\n",
    "\n",
    "large_df_url = \"yellow_tripdata_2011-*.parquet\"\n",
    "koalas_data = spark.read.parquet(large_df_url)\n",
    "\n",
    "grouped_stats = groupby_statistics(koalas_data)\n",
    "other_large_pd = grouped_stats.toPandas()\n",
    "other_large = spark.createDataFrame(other_large_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjZjTHP881fQ"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(large_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kurdbvW_81fU",
    "outputId": "a9b28dfe-3594-45e6-b49b-534b74f01a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.07800000000042928 seconds\n",
      "count took: 0.1570000000001528 seconds\n",
      "mean took: 0.6089999999994689 seconds\n",
      "standard deviation took: 0.6559999999999491 seconds\n",
      "mean of columns addition took: 0.9380000000001019 seconds\n",
      "addition of columns took: 0.0 seconds\n",
      "mean of columns multiplication took: 0.9369999999998981 seconds\n",
      "multiplication of columns took: 0.0 seconds\n",
      "value counts took: 0.016000000000531145 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.03099999999994907 seconds\n",
      "join count took: 10.54699999999957 seconds\n",
      "join took: 0.016000000000531145 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=koalas_large_df_benchmarks, name='read file')\n",
    "benchmark(count, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='count')\n",
    "benchmark(mean, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=koalas_data, benchmarks=koalas_large_df_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, koalas_data, benchmarks=koalas_large_df_benchmarks, name='join count', other=other_large)\n",
    "benchmark(join_data, koalas_data, benchmarks=koalas_large_df_benchmarks, name='join', other=other_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnJXs_1X81fU"
   },
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyJF2zuT81fU"
   },
   "source": [
    "##### Regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_gMx5lW81fU"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(regular_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IC8W8fGG81fU"
   },
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Szw2wkO81fU",
    "outputId": "31f6bd3d-e4ff-438b-d2d4-261ec2edf93f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.21900000000005093 seconds\n",
      "filtered mean took: 0.34400000000005093 seconds\n",
      "filtered standard deviation took: 0.375 seconds\n",
      "filtered mean of columns addition took: 0.31199999999989814 seconds\n",
      "filtered addition of columns took: 0.0 seconds\n",
      "filtered mean of columns multiplication took: 0.3279999999995198 seconds\n",
      "filtered multiplication of columns took: 0.0 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.016000000000531145 seconds\n",
      "filtered groupby statistics took: 0.03099999999994907 seconds\n",
      "filtered join took: 0.016000000000531145 seconds\n",
      "filtered join count took: 10.45299999999952 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered count')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other_pd = groupby_statistics(koalas_filtered).toPandas()\n",
    "other_pd.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in other_pd.columns]\n",
    "other = spark.createDataFrame(other_pd)\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eadorwsL81fU"
   },
   "source": [
    "##### Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbQ8mh7881fV"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(small_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlxbwR4Z81fV"
   },
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlKLp3yH81fV",
    "outputId": "946cda8a-3611-4453-9165-38bc1606d1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.125 seconds\n",
      "filtered mean took: 0.18800000000010186 seconds\n",
      "filtered standard deviation took: 0.15599999999994907 seconds\n",
      "filtered mean of columns addition took: 0.14100000000053114 seconds\n",
      "filtered addition of columns took: 0.0 seconds\n",
      "filtered mean of columns multiplication took: 0.15599999999994907 seconds\n",
      "filtered multiplication of columns took: 0.0 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.01599999999962165 seconds\n",
      "filtered groupby statistics took: 0.046000000000276486 seconds\n",
      "filtered join took: 0.014999999999417923 seconds\n",
      "filtered join count took: 10.532000000000153 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered count')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other_pd = groupby_statistics(koalas_filtered).toPandas()\n",
    "other_pd.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in other_pd.columns]\n",
    "other = spark.createDataFrame(other_pd)\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqYdN_SF81fV"
   },
   "source": [
    "##### Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrrIeZrS81fV"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(large_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvSXdQ5V81fV"
   },
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_ktwfSQ81fV",
    "outputId": "8b75875a-b72a-4c60-83fd-88d15f353644"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.75 seconds\n",
      "filtered mean took: 1.2039999999997235 seconds\n",
      "filtered standard deviation took: 1.0 seconds\n",
      "filtered mean of columns addition took: 1.0 seconds\n",
      "filtered addition of columns took: 0.0 seconds\n",
      "filtered mean of columns multiplication took: 1.0 seconds\n",
      "filtered multiplication of columns took: 0.0 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.015000000000327418 seconds\n",
      "filtered groupby statistics took: 0.03099999999994907 seconds\n",
      "filtered join took: 0.0 seconds\n",
      "filtered join count took: 11.108999999999469 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered count')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other_pd = groupby_statistics(koalas_filtered).toPandas()\n",
    "other_pd.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in other_pd.columns]\n",
    "other = spark.createDataFrame(other_pd)\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_large_df_benchmarks, name='filtered join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWkkHP4v81fW"
   },
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmXUX_6D81fW"
   },
   "source": [
    "##### Regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I48orSVS81fW"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(regular_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K29zE2bz81fW"
   },
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6C2vwxP81fW",
    "outputId": "1d581cfd-61e2-4ce9-9f2c-6e5034a2c677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce caching: 508740 rows of filtered data\n"
     ]
    }
   ],
   "source": [
    "koalas_filtered_cached = koalas_filtered.cache()\n",
    "print(f'Enforce caching: {koalas_filtered_cached.count()} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMhg-_IN81fW",
    "outputId": "b0b17fd9-eb4d-4a8c-f155-990f99b14d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered and cached count took: 0.125 seconds\n",
      "filtered and cached mean took: 0.1570000000001528 seconds\n",
      "filtered and cached standard deviation took: 0.18699999999989814 seconds\n",
      "filtered and cached mean of columns addition took: 0.21900000000005093 seconds\n",
      "filtered and cached addition of columns took: 0.01599999999962165 seconds\n",
      "filtered and cached mean of columns multiplication took: 0.20300000000042928 seconds\n",
      "filtered and cached multiplication of columns took: 0.0 seconds\n",
      "filtered and cached mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached value counts took: 0.03099999999994907 seconds\n",
      "filtered and cached groupby statistics took: 0.03099999999994907 seconds\n",
      "filtered and cached join took: 0.01599999999962165 seconds\n",
      "filtered and cached join count took: 11.17200000000048 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached count')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other_pd = groupby_statistics(koalas_filtered).toPandas()\n",
    "other_pd.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in other_pd.columns]\n",
    "other = spark.createDataFrame(other_pd)\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_regular_df_benchmarks, name='filtered and cached join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ruki7NVG81fW"
   },
   "source": [
    "##### Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hj-kvZWn81fX"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(small_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGkl-QyO81fX"
   },
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7es48-881fX",
    "outputId": "6aab6ca0-8f1c-4981-ff01-ec8d2c2f66ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce caching: 15100 rows of filtered data\n"
     ]
    }
   ],
   "source": [
    "koalas_filtered_cached = koalas_filtered.cache()\n",
    "print(f'Enforce caching: {koalas_filtered_cached.count()} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2Vyutl781fX",
    "outputId": "f4d0854e-bce4-487f-d77f-8502b4806028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered and cached count took: 0.09400000000005093 seconds\n",
      "filtered and cached mean took: 0.07899999999972351 seconds\n",
      "filtered and cached standard deviation took: 0.125 seconds\n",
      "filtered and cached mean of columns addition took: 0.1710000000002765 seconds\n",
      "filtered and cached addition of columns took: 0.0 seconds\n",
      "filtered and cached mean of columns multiplication took: 0.125 seconds\n",
      "filtered and cached multiplication of columns took: 0.0 seconds\n",
      "filtered and cached mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached value counts took: 0.01599999999962165 seconds\n",
      "filtered and cached groupby statistics took: 0.03099999999994907 seconds\n",
      "filtered and cached join took: 0.01599999999962165 seconds\n",
      "filtered and cached join count took: 12.42200000000048 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached count')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other_pd = groupby_statistics(koalas_filtered).toPandas()\n",
    "other_pd.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in other_pd.columns]\n",
    "other = spark.createDataFrame(other_pd)\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_small_df_benchmarks, name='filtered and cached join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL-Vy6FV81fX"
   },
   "source": [
    "##### Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmRkr39I81fX"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(large_df_url)\n",
    "\n",
    "koalas_data = read_file_parquet()\n",
    "\n",
    "koalas_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFdgAan381fX"
   },
   "outputs": [],
   "source": [
    "expr_filter = (koalas_data.tip_amount >= 1) & (koalas_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "koalas_filtered = filter_data(koalas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8xjgy2y81fX",
    "outputId": "ab347945-3c2d-4892-eb74-25dda13c26e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enforce caching: 4826147 rows of filtered data\n"
     ]
    }
   ],
   "source": [
    "koalas_filtered_cached = koalas_filtered.cache()\n",
    "print(f'Enforce caching: {koalas_filtered_cached.count()} rows of filtered data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naXK-Ati81fY",
    "outputId": "b948dc44-59a1-4502-9fa9-24958209b762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered and cached count took: 2.233999999999469 seconds\n",
      "filtered and cached mean took: 2.860000000000582 seconds\n",
      "filtered and cached standard deviation took: 3.092999999999847 seconds\n",
      "filtered and cached mean of columns addition took: 3.032000000000153 seconds\n",
      "filtered and cached addition of columns took: 0.0 seconds\n",
      "filtered and cached mean of columns multiplication took: 2.592999999999847 seconds\n",
      "filtered and cached multiplication of columns took: 0.0 seconds\n",
      "filtered and cached mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached complex arithmetic ops took: 0.0 seconds\n",
      "filtered and cached value counts took: 0.032000000000152795 seconds\n",
      "filtered and cached groupby statistics took: 0.03099999999994907 seconds\n",
      "filtered and cached join took: 0.01599999999962165 seconds\n",
      "filtered and cached join count took: 13.780999999999949 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached count')\n",
    "benchmark(mean, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, koalas_filtered, benchmarks=koalas_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other_pd = groupby_statistics(koalas_filtered).toPandas()\n",
    "other_pd.columns = [f\"{c[0]}_{c[1]}\" if isinstance(c, tuple) else c for c in other_pd.columns]\n",
    "other = spark.createDataFrame(other_pd)\n",
    "\n",
    "benchmark(join_data, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached join', other=other)\n",
    "benchmark(join_count, koalas_filtered, benchmarks=koalas_benchmarks, name='filtered and cached join count', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AAdnNgs81fY"
   },
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1EVZJ5D81fY",
    "outputId": "5bb3ac25-0fdc-4cd8-8dc1-d151ea329a13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 54798 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 16:57:02,954 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle db41d42f268098f89cd7d93415ebb8b6 initialized by task ('shuffle-transfer-db41d42f268098f89cd7d93415ebb8b6', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 16:57:04,594 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle db41d42f268098f89cd7d93415ebb8b6 deactivated due to stimulus 'task-finished-1748534224.591795'\n",
      "2025-05-29 16:57:13,635 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e854cb1368ab2ae1e558ab63a0930805 initialized by task ('shuffle-transfer-e854cb1368ab2ae1e558ab63a0930805', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 16:57:14,770 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle e854cb1368ab2ae1e558ab63a0930805 deactivated due to stimulus 'task-finished-1748534234.769863'\n",
      "2025-05-29 16:59:23,239 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b3b21883935601605b95ef6ae6a2fd14 initialized by task ('shuffle-transfer-b3b21883935601605b95ef6ae6a2fd14', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 16:59:23,291 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b3b21883935601605b95ef6ae6a2fd14 deactivated due to stimulus 'task-finished-1748534363.2896173'\n",
      "2025-05-29 17:00:15,349 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b3b21883935601605b95ef6ae6a2fd14 initialized by task ('shuffle-transfer-b3b21883935601605b95ef6ae6a2fd14', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 17:00:17,031 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b3b21883935601605b95ef6ae6a2fd14 deactivated due to stimulus 'task-finished-1748534417.0294518'\n",
      "2025-05-29 17:00:56,286 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle fdc97506a54afd61718a1b7ff2dc1602 initialized by task ('shuffle-transfer-fdc97506a54afd61718a1b7ff2dc1602', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 17:00:56,345 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle fdc97506a54afd61718a1b7ff2dc1602 deactivated due to stimulus 'task-finished-1748534456.344721'\n",
      "2025-05-29 17:01:06,431 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 33cb1c8e24a90b9792061553eb932bbb initialized by task ('shuffle-transfer-33cb1c8e24a90b9792061553eb932bbb', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 17:01:06,602 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 33cb1c8e24a90b9792061553eb932bbb deactivated due to stimulus 'task-finished-1748534466.6010616'\n",
      "2025-05-29 17:04:56,829 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle f6c2fa078c18b8284ef59d1ada423866 initialized by task ('shuffle-transfer-f6c2fa078c18b8284ef59d1ada423866', 0) executed on worker tcp://127.0.0.1:54823\n",
      "2025-05-29 17:04:56,899 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle f6c2fa078c18b8284ef59d1ada423866 deactivated due to stimulus 'task-finished-1748534696.898369'\n",
      "2025-05-29 17:05:05,101 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:54823 (pid=13924) exceeded 95% memory budget. Restarting...\n",
      "2025-05-29 17:05:05,820 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:05:11,382 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:54822 (pid=3568) exceeded 95% memory budget. Restarting...\n",
      "2025-05-29 17:05:12,021 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:07:03,623 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:54830 (pid=17260) exceeded 95% memory budget. Restarting...\n",
      "2025-05-29 17:07:04,466 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:07:11,355 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:55191 (pid=16232) exceeded 95% memory budget. Restarting...\n",
      "2025-05-29 17:07:11,944 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:07:16,654 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:55182 (pid=17528) exceeded 95% memory budget. Restarting...\n",
      "2025-05-29 17:07:17,239 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:07:23,096 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:55467 (pid=4664) exceeded 95% memory budget. Restarting...\n",
      "2025-05-29 17:07:23,117 - distributed.scheduler - ERROR - Task ('read_parquet-880d90cb855161ccab6f8dc3328392d7', 0) marked as failed because 4 workers died while trying to run it\n",
      "2025-05-29 17:07:23,691 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:10:39,833 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:10:39,835 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:10:39,841 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-05-29 17:10:40,205 - distributed.nanny - WARNING - Restarting worker\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=PermissionError(13, 'Acesso negado', None, 5, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\process.py\", line 55, in _call_and_set_future\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py\", line 140, in kill\n",
      "    self._popen.kill()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py\", line 123, in terminate\n",
      "    _winapi.TerminateProcess(int(self._handle), TERMINATE)\n",
      "PermissionError: [WinError 5] Acesso negado\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=PermissionError(13, 'Acesso negado', None, 5, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\process.py\", line 55, in _call_and_set_future\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py\", line 140, in kill\n",
      "    self._popen.kill()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py\", line 123, in terminate\n",
      "    _winapi.TerminateProcess(int(self._handle), TERMINATE)\n",
      "PermissionError: [WinError 5] Acesso negado\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=PermissionError(13, 'Acesso negado', None, 5, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\process.py\", line 55, in _call_and_set_future\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py\", line 140, in kill\n",
      "    self._popen.kill()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py\", line 123, in terminate\n",
      "    _winapi.TerminateProcess(int(self._handle), TERMINATE)\n",
      "PermissionError: [WinError 5] Acesso negado\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=PermissionError(13, 'Acesso negado', None, 5, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\process.py\", line 55, in _call_and_set_future\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py\", line 140, in kill\n",
      "    self._popen.kill()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py\", line 123, in terminate\n",
      "    _winapi.TerminateProcess(int(self._handle), TERMINATE)\n",
      "PermissionError: [WinError 5] Acesso negado\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=PermissionError(13, 'Acesso negado', None, 5, None)>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\process.py\", line 55, in _call_and_set_future\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\process.py\", line 140, in kill\n",
      "    self._popen.kill()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\multiprocessing\\popen_spawn_win32.py\", line 123, in terminate\n",
      "    _winapi.TerminateProcess(int(self._handle), TERMINATE)\n",
      "PermissionError: [WinError 5] Acesso negado\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster(memory_limit='6GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH91dTwv81fY"
   },
   "source": [
    "#### Standard operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2EYEQ8I81fY"
   },
   "outputs": [],
   "source": [
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    "\n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg({\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std']\n",
    "    })\n",
    "\n",
    "dask_data = dd.read_parquet(regular_df_url)\n",
    "other = groupby_statistics(dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z31itHk581fY"
   },
   "source": [
    "##### Regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohsjKCiq81fZ"
   },
   "outputs": [],
   "source": [
    "dask_regular_df_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXJmAJo381fZ"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(regular_df_url)\n",
    "\n",
    "dask_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQN8C6sb81fZ",
    "outputId": "055c2ed8-304e-438d-de32-b1a53933925a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.031000000000858563 seconds\n",
      "count took: 0.01599999999962165 seconds\n",
      "count index length took: 1.8119999999998981 seconds\n",
      "mean took: 0.21899999999914144 seconds\n",
      "standard deviation took: 0.18800000000010186 seconds\n",
      "mean of columns addition took: 0.23400000000037835 seconds\n",
      "addition of columns took: 0.18800000000010186 seconds\n",
      "mean of columns multiplication took: 0.2029999999995198 seconds\n",
      "multiplication of columns took: 0.18699999999989814 seconds\n",
      "value counts took: 1.8130000000001019 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.0 seconds\n",
      "join count took: 0.5930000000007567 seconds\n",
      "join took: 1.2819999999992433 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_regular_df_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_regular_df_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_regular_df_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_regular_df_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMkpaEkT81fZ"
   },
   "source": [
    "##### Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsrdj3Uc81fZ"
   },
   "outputs": [],
   "source": [
    "dask_small_df_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq7mnY3_81fZ"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(small_df_url)\n",
    "\n",
    "dask_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6Aavc5P81fZ",
    "outputId": "874bf8dc-8590-4e05-f817-4a961398284e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.01599999999962165 seconds\n",
      "count took: 0.01599999999962165 seconds\n",
      "count index length took: 0.07799999999951979 seconds\n",
      "mean took: 0.04700000000048021 seconds\n",
      "standard deviation took: 0.046000000000276486 seconds\n",
      "mean of columns addition took: 0.06300000000010186 seconds\n",
      "addition of columns took: 0.06199999999989814 seconds\n",
      "mean of columns multiplication took: 0.07899999999972351 seconds\n",
      "multiplication of columns took: 0.046000000000276486 seconds\n",
      "value counts took: 1.2039999999997235 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.031000000000858563 seconds\n",
      "join count took: 0.43699999999989814 seconds\n",
      "join took: 0.4069999999992433 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_small_df_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_small_df_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_small_df_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_small_df_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_small_df_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_small_df_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_small_df_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_small_df_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_small_df_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_small_df_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_small_df_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_small_df_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_small_df_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_small_df_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_small_df_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ17ochF81fZ"
   },
   "source": [
    "##### Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4o8phxy81fa"
   },
   "outputs": [],
   "source": [
    "dask_large_df_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqhgCSZp81fa"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(large_df_url)\n",
    "\n",
    "dask_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qexg3Poz81fa",
    "outputId": "beb4398a-5d40-4536-dd77-e0ad7a1054bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.015000000001236913 seconds\n",
      "count took: 0.01599999999962165 seconds\n",
      "count index length took: 0.030999999999039574 seconds\n",
      "mean took: 0.6560000000008586 seconds\n",
      "standard deviation took: 0.9850000000005821 seconds\n",
      "mean of columns addition took: 1.0939999999991414 seconds\n",
      "addition of columns took: 1.5619999999998981 seconds\n",
      "mean of columns multiplication took: 1.0630000000001019 seconds\n",
      "multiplication of columns took: 1.4369999999998981 seconds\n",
      "value counts took: 2.561999999999898 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.01600000000144064 seconds\n",
      "join count took: 0.6719999999986612 seconds\n",
      "join took: 26.968000000000757 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_large_df_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_large_df_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_data, benchmarks=dask_large_df_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_large_df_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_large_df_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_large_df_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_large_df_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_large_df_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_large_df_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_large_df_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_large_df_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_large_df_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_large_df_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_large_df_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_large_df_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UJX-dHw81fa"
   },
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Xw4ozRD81fa"
   },
   "source": [
    "##### Regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mti6s0HQ81fa"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(regular_df_url)\n",
    "\n",
    "dask_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWo2TSkT81fa"
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da-i8_aW81fa",
    "outputId": "428a649c-cc1b-4ae1-e5b8-21be4b04197a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.2039999999997235 seconds\n",
      "filtered count index length took: 0.1710000000002765 seconds\n",
      "filtered mean took: 0.21899999999914144 seconds\n",
      "filtered standard deviation took: 0.26600000000144064 seconds\n",
      "filtered mean of columns addition took: 0.21899999999914144 seconds\n",
      "filtered addition of columns took: 0.25 seconds\n",
      "filtered mean of columns multiplication took: 0.18699999999989814 seconds\n",
      "filtered multiplication of columns took: 0.25 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.2970000000004802 seconds\n",
      "filtered groupby statistics took: 0.01599999999962165 seconds\n",
      "filtered join count took: 0.4850000000005821 seconds\n",
      "filtered join took: 1.3590000000003783 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_regular_df_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_regular_df_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_regular_df_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_regular_df_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYQjOXDT81fa"
   },
   "source": [
    "##### Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25CgEyUR81fb"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(small_df_url)\n",
    "\n",
    "dask_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMltxULQ81fb"
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCIJf6NG81fb",
    "outputId": "81728c98-d83d-4aa1-a422-22aaa132d13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.09399999999914144 seconds\n",
      "filtered count index length took: 0.0930000000007567 seconds\n",
      "filtered mean took: 0.07899999999972351 seconds\n",
      "filtered standard deviation took: 0.0930000000007567 seconds\n",
      "filtered mean of columns addition took: 0.125 seconds\n",
      "filtered addition of columns took: 0.10999999999876309 seconds\n",
      "filtered mean of columns multiplication took: 0.15600000000085856 seconds\n",
      "filtered multiplication of columns took: 0.07799999999951979 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.2970000000004802 seconds\n",
      "filtered groupby statistics took: 0.01599999999962165 seconds\n",
      "filtered join count took: 0.23500000000058208 seconds\n",
      "filtered join took: 0.21899999999914144 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_small_df_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_small_df_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_small_df_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_small_df_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy48080W81fb"
   },
   "source": [
    "#### Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlcllmdw81fb"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(large_df_url)\n",
    "\n",
    "dask_data = read_file_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5zl2ZVv81fb"
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW7Jd6nf81fb",
    "outputId": "f72602e9-221f-4de7-fb61-34854b4da42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.8430000000007567 seconds\n",
      "filtered count index length took: 0.0319999999992433 seconds\n",
      "filtered mean took: 1.25 seconds\n",
      "filtered standard deviation took: 1.5619999999998981 seconds\n",
      "filtered mean of columns addition took: 1.25 seconds\n",
      "filtered addition of columns took: 1.7810000000008586 seconds\n",
      "filtered mean of columns multiplication took: 1.2189999999991414 seconds\n",
      "filtered multiplication of columns took: 1.735000000000582 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 1.375 seconds\n",
      "filtered groupby statistics took: 0.01600000000144064 seconds\n",
      "filtered join count took: 2.3289999999997235 seconds\n",
      "filtered join took: 28.03100000000086 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_large_df_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_large_df_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_large_df_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_large_df_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpN1u2iw81fb"
   },
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLPRnnnx81fb",
    "outputId": "682afb34-987d-43d4-c994-2abf111a726c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting until all futures are finished\n",
      "All futures are finished\n"
     ]
    }
   ],
   "source": [
    "dask_filtered = client.persist(dask_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_filtered)\n",
    "print('All futures are finished')\n",
    "\n",
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzRJbymV81fc",
    "outputId": "950cd7a2-b980-4c09-c493-ca6dc2088d0c"
   },
   "outputs": [
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('read_parquet-880d90cb855161ccab6f8dc3328392d7', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:55467. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdask_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdask_benchmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiltered and cached count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m benchmark(count_index_length, dask_filtered, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered and cached count index length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m benchmark(mean, dask_filtered, benchmarks\u001b[38;5;241m=\u001b[39mdask_benchmarks, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered and cached mean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m, in \u001b[0;36mbenchmark\u001b[1;34m(f, df, benchmarks, name, **kwargs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark\u001b[39m(f, df, benchmarks, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m----> 4\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      5\u001b[0m     benchmarks[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[0;32m      6\u001b[0m     benchmarks[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "Cell \u001b[1;32mIn[108], line 2\u001b[0m, in \u001b[0;36mcount\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:391\u001b[0m, in \u001b[0;36mFrameBase.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\dask\\base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\dask\\base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[0;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[1;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(expr, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\distributed\\client.py:2409\u001b[0m, in \u001b[0;36mClient._gather\u001b[1;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[0;32m   2407\u001b[0m     exception \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mexception\n\u001b[0;32m   2408\u001b[0m     traceback \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mtraceback\n\u001b[1;32m-> 2409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m   2410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2411\u001b[0m     bad_keys\u001b[38;5;241m.\u001b[39madd(key)\n",
      "\u001b[1;31mKilledWorker\u001b[0m: Attempted to run task ('read_parquet-880d90cb855161ccab6f8dc3328392d7', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:55467. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached count')\n",
    "benchmark(count_index_length, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached count index length')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered and cached value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered and cached join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKOFaDlJ81fc"
   },
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IIG69h581fc"
   },
   "source": [
    "## EXPERIMENT 2: Running datasets with different combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vVddHf081fc"
   },
   "source": [
    "### Dask + JobLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBVxpTav81fc"
   },
   "source": [
    "#### Regular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_znRys681fc"
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PouEZKHs81fc"
   },
   "outputs": [],
   "source": [
    "large_dask_joblib_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XF-f4a_81fc"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet():\n",
    "    return pd.read_parquet(large_df_url)\n",
    "\n",
    "joblib_dask_data = read_file_parquet()\n",
    "\n",
    "def count(df):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    return df.fare_amount + df.tip_amount\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    return df.fare_amount * df.tip_amount\n",
    "\n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2 +\n",
    "            np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) *\n",
    "            np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "\n",
    "    return np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1 - temp)), 2)\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    values = [complicated_arithmetic_operation(df) for _ in range(1000)]\n",
    "    return np.mean(values)\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg({\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std']\n",
    "    })\n",
    "    return gb\n",
    "\n",
    "other = groupby_statistics(joblib_dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(pd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return pd.merge(df, other, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGezvwwd81fd",
    "outputId": "fe2bf228-9f01-4413-85f1-cf8eb29da35b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count took: 0.0 seconds\n",
      "count index length took: 0.0 seconds\n",
      "mean took: 0.07800000000133878 seconds\n",
      "standard deviation took: 0.31199999999989814 seconds\n",
      "mean of columns addition took: 0.1569999999992433 seconds\n",
      "addition of columns took: 0.07799999999951979 seconds\n",
      "mean of columns multiplication took: 0.125 seconds\n",
      "multiplication of columns took: 0.07800000000133878 seconds\n",
      "value counts took: 0.26599999999962165 seconds\n",
      "mean of complex arithmetic ops took: 0.031000000000858563 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 0.625 seconds\n",
      "join count took: 0.06300000000010186 seconds\n",
      "join took: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='count index length')\n",
    "benchmark(mean, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, joblib_dask_data, benchmarks=large_dask_joblib_benchmarks, name='join', other=other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqQdI7Bt81fd"
   },
   "source": [
    "### Dask + Modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxGl5QNB81fd",
    "outputId": "80126c6f-c343-4b61-afee-6f784d69ac95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modin version: 0.32.0\n"
     ]
    }
   ],
   "source": [
    "import modin.pandas as mpd\n",
    "from modin.config import Engine\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"\n",
    "\n",
    "print('modin version: %s' % mpd.__version__)\n",
    "\n",
    "Engine.put(\"dask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpEkSB8m81fd"
   },
   "outputs": [],
   "source": [
    "dask_modin_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPCtElfP81fd"
   },
   "source": [
    "#### Standard Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIKVbmOH81fd"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return mpd.read_parquet(large_df_url)\n",
    "\n",
    "dask_modin_data = read_file_parquet()\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = groupby_statistics(dask_modin_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(mpd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return mpd.merge(df, other, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oDyOwEX81fd",
    "outputId": "30fc0eea-68c0-4117-8f31-66d2588b05f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 1.1719999999913853 seconds\n",
      "count took: 0.0 seconds\n",
      "count index length took: 0.0 seconds\n",
      "mean took: 0.10899999999674037 seconds\n",
      "standard deviation took: 0.15600000001722947 seconds\n",
      "mean of columns addition took: 0.1870000000053551 seconds\n",
      "addition of columns took: 0.03200000000651926 seconds\n",
      "mean of columns multiplication took: 0.375 seconds\n",
      "multiplication of columns took: 0.046000000002095476 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `df.groupby(categorical_by, sort=False)` implementation has mismatches with pandas:\n",
      "the groupby keys will be sorted anyway, although the 'sort=False' was passed. See the following issue for more details: https://github.com/modin-project/modin/issues/3571.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value counts took: 0.42199999999138527 seconds\n",
      "mean of complex arithmetic ops took: 0.0 seconds\n",
      "complex arithmetic ops took: 0.0 seconds\n",
      "groupby statistics took: 3.062999999994645 seconds\n",
      "join count took: 2.9690000000118744 seconds\n",
      "join took: 2.812000000005355 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_modin_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_modin_data, benchmarks=dask_modin_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_modin_data, benchmarks=dask_modin_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_modin_data, benchmarks=dask_modin_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezcdPZ8U81fd"
   },
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLa7JxRz81fe"
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_modin_data.tip_amount >= 1) & (dask_modin_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_modin_filtered = filter_data(dask_modin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFBtb_3n81fe",
    "outputId": "0075583c-3037-473b-f1f7-b586587064ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.09399999998277053 seconds\n",
      "filtered count index length took: 0.0 seconds\n",
      "filtered mean took: 0.07800000000861473 seconds\n",
      "filtered standard deviation took: 0.125 seconds\n",
      "filtered mean of columns addition took: 0.1870000000053551 seconds\n",
      "filtered addition of columns took: 0.046999999991385266 seconds\n",
      "filtered mean of columns multiplication took: 0.235000000015134 seconds\n",
      "filtered multiplication of columns took: 0.030999999988125637 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.7179999999934807 seconds\n",
      "filtered groupby statistics took: 1.1410000000032596 seconds\n",
      "filtered join count took: 1.0780000000086147 seconds\n",
      "filtered join took: 1.0780000000086147 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_modin_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFvmn05n81fe"
   },
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06s_rkxm81fe",
    "outputId": "21acc371-9824-432c-c9a4-76f2de795197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: finished, type: pandas.core.frame.DataFrame, key: ('getitem-2951f6bd6cf190ee8e7888115d853521', 0)>}, not_done=set())"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_parquet(large_df_url)\n",
    "filtered = df[df.fare_amount > 10]\n",
    "filtered = client.persist(filtered)\n",
    "wait(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANcMYWHX81fe",
    "outputId": "3c7855ef-473d-413f-ff61-32d6325e5099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 0.0 seconds\n",
      "filtered count index length took: 0.0 seconds\n",
      "filtered mean took: 0.10899999999674037 seconds\n",
      "filtered standard deviation took: 0.07800000000861473 seconds\n",
      "filtered mean of columns addition took: 0.15599999998812564 seconds\n",
      "filtered addition of columns took: 0.0629999999946449 seconds\n",
      "filtered mean of columns multiplication took: 0.21900000001187436 seconds\n",
      "filtered multiplication of columns took: 0.030999999988125637 seconds\n",
      "filtered mean of complex arithmetic ops took: 0.0 seconds\n",
      "filtered complex arithmetic ops took: 0.0 seconds\n",
      "filtered value counts took: 0.40600000001722947 seconds\n",
      "filtered groupby statistics took: 1.1410000000032596 seconds\n",
      "filtered join count took: 1.125 seconds\n",
      "filtered join took: 1.062000000005355 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark(count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_modin_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_modin_filtered, benchmarks=dask_modin_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esaJLjjl81fe"
   },
   "source": [
    "### Dask + Rapids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWGotTyT81fe",
    "outputId": "9695a465-5258-48e2-9783-736046711a8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x1a23e0f7910>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7S22YgX81fe"
   },
   "outputs": [],
   "source": [
    "dask_rapids_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8RVqmzZ81ff"
   },
   "source": [
    "#### Standard Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMPVjRHd81ff"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(large_df_url)\n",
    "\n",
    "dask_rapids_data = read_file_parquet()\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = groupby_statistics(dask_rapids_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hnc-qEs281ff"
   },
   "outputs": [],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_rapids_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_rapids_data, benchmarks=dask_rapids_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAC2JRKh81ff"
   },
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdcUqLIN81ff"
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_rapids_data.tip_amount >= 1) & (dask_rapids_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_rapids_filtered = filter_data(dask_rapids_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Esu5nvC-81ff"
   },
   "outputs": [],
   "source": [
    "benchmark(count, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_rapids_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOx1GBR181ff"
   },
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDlgp_nC81fg"
   },
   "outputs": [],
   "source": [
    "dask_rapids_filtered = client.persist(dask_rapids_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_rapids_filtered)\n",
    "print('All futures are finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcXwNome81fg"
   },
   "outputs": [],
   "source": [
    "benchmark(count, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_rapids_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_rapids_filtered, benchmarks=dask_rapids_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JzeKRkL81fg"
   },
   "source": [
    "### Dask + Modin + Rapids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOHG0i_v81fg"
   },
   "outputs": [],
   "source": [
    "import modin.pandas as mpd\n",
    "from modin.config import Engine\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "Engine.put(\"dask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwz79-KE81fg"
   },
   "outputs": [],
   "source": [
    "dask_modin_rapids_data = mpd.read_parquet(\"data.parquet\",paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbKfeVvM81fg"
   },
   "outputs": [],
   "source": [
    "dask_modin_rapids_benchmarks = {\n",
    "    'duration': [],     # in seconds\n",
    "    'task': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI4MgxG781fg"
   },
   "source": [
    "#### Standard Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3XPsLZk81fg"
   },
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return mpd.read_parquet(large_df_url)\n",
    "\n",
    "def count(df=None):\n",
    "    return len(df)\n",
    "\n",
    "def count_index_length(df=None):\n",
    "    return len(df.index)\n",
    "\n",
    "def mean(df):\n",
    "    return df.fare_amount.mean()\n",
    "\n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std()\n",
    "\n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean()\n",
    "\n",
    "def sum_columns(df):\n",
    "    x = df.fare_amount + df.tip_amount\n",
    "    return x\n",
    "\n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean()\n",
    "\n",
    "def product_columns(df):\n",
    "    x = df.fare_amount * df.tip_amount\n",
    "    return x\n",
    "\n",
    "def value_counts(df):\n",
    "    val_counts = df.fare_amount.value_counts()\n",
    "    return val_counts\n",
    "\n",
    "\n",
    "#   In the original experiment, the following two functions used the longitude and latitude values of the pickup and the dropout places.\n",
    "#   Since the datasets provided by NYC TLC Trip Record Data no longer have longitude and latitude values (only the pickup and dropout places IDs),\n",
    "# we used arbitrary longitude and latitude values. The goal of this experiment is to compare the computational cost of the calculations, hence\n",
    "# the values are not relevant.\n",
    "def complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    start_lon,end_lon = np.random.randint(-180,180),np.random.randint(-180,180)\n",
    "    start_lat,end_lat = np.random.randint(-90,90),np.random.randint(-90,90)\n",
    "\n",
    "    theta_1 = start_lon\n",
    "    phi_1 = start_lat\n",
    "    theta_2 = end_lon\n",
    "    phi_2 = end_lat\n",
    "    temp = (np.sin((theta_2 - theta_1) / 2 * np.pi / 180) ** 2\n",
    "           + np.cos(theta_1 * np.pi / 180) * np.cos(theta_2 * np.pi / 180) * np.sin((phi_2 - phi_1) / 2 * np.pi / 180) ** 2)\n",
    "    ret = np.multiply(np.arctan2(np.sqrt(temp), np.sqrt(1-temp)),2)\n",
    "    return ret.mean()\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'],\n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    )\n",
    "    return gb\n",
    "\n",
    "other = groupby_statistics(dask_modin_rapids_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    "\n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vH5M6QLP81fg"
   },
   "outputs": [],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_modin_rapids_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='count')\n",
    "benchmark(count_index_length, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='count index length')\n",
    "benchmark(mean, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_modin_rapids_data, benchmarks=dask_modin_rapids_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kxoZoPN81fh"
   },
   "source": [
    "#### Operations with filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTbVdv3381fh"
   },
   "outputs": [],
   "source": [
    "expr_filter = (dask_modin_rapids_data.tip_amount >= 1) & (dask_modin_rapids_data.tip_amount <= 5)\n",
    "\n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "\n",
    "dask_modin_rapids_filtered = filter_data(dask_modin_rapids_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4oa7X5I81fh"
   },
   "outputs": [],
   "source": [
    "benchmark(count, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_modin_rapids_filtered)\n",
    "other.columns = pd.Index([e[0] +'_'+ e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0f0t76781fh"
   },
   "source": [
    "#### Operations with filtering and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fcd9XmId81fh"
   },
   "outputs": [],
   "source": [
    "dask_modin_rapids_filtered = client.persist(dask_modin_rapids_filtered)\n",
    "\n",
    "from distributed import wait\n",
    "print('Waiting until all futures are finished')\n",
    "wait(dask_modin_rapids_filtered)\n",
    "print('All futures are finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Lb_eOMQ81fh"
   },
   "outputs": [],
   "source": [
    "benchmark(count, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered count')\n",
    "benchmark(count_index_length, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered count index length')\n",
    "benchmark(mean, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered groupby statistics')\n",
    "\n",
    "other = groupby_statistics(dask_modin_rapids_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    "\n",
    "benchmark(join_count, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_modin_rapids_filtered, benchmarks=dask_modin_rapids_benchmarks, name='filtered join', other=other)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
